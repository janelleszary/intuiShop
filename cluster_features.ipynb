{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/both_vectors.csv')\n",
    "allvec = df1\n",
    "id_vec = df1[df1.columns[0:3]]\n",
    "text_vec = df1[df1.columns[3:303]]\n",
    "image_vec = df1[df1.columns[303:4398]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hand-labeled categories\n",
    "hand_df = pd.read_csv('data/lookup_table.csv', delimiter=',')\n",
    "hand = []\n",
    "for i in range(0, len(id_vec)):\n",
    "    a = hand_df[hand_df.SS==id_vec.category[i].strip()].HAND\n",
    "    if (len(a)>0):\n",
    "        hand.append(a.values[0])\n",
    "    else:\n",
    "        hand.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "handdf = pd.DataFrame()\n",
    "handdf['hand'] = hand\n",
    "# Get rid of NA rows\n",
    "all_withhand = pd.concat((id_vec, handdf, text_vec, image_vec), axis=1)\n",
    "all_withhand = all_withhand[all_withhand.hand != 'NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get new subsets\n",
    "Hid_vec = all_withhand[all_withhand.columns[0:4]]\n",
    "Htext_vec = all_withhand[all_withhand.columns[4:304]]\n",
    "Himage_vec = all_withhand[all_withhand.columns[304:4399]]\n",
    "len(np.unique(all_withhand.hand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1205dd390>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD/CAYAAAAaGBpmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAF7JJREFUeJzt3X/QXNV93/H3F8lWwERYwBOBJYzUWjYDJP7BE6LYac0E\ntyiDazEppkqaILsUTQcC1MmMkep2SNsoltuMUzMJahVjkIgNUXEa1NjCYGHXTRshhCEIIQgKQkiK\nAAVsSJoWW+TbP+5RfNmzq0fP7iM9K+n9mrmzd889556zq33uZ++5u6vITCRJajthsgcgSRo+hoMk\nqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqTJ1sgfQr9NPPz3nzJkz2cOQpKPK\nww8//BeZOTJWvTHDISK+AHwYeDEzzy9l/xH4R8D3gD8DPp6Z3y3blgFXAa8D12fm10r5BcDtwInA\nV4EbMjMjYhqwBrgAeAn4J5n57FjjmjNnDps3bx6rmiSpJSJ2Hkq9Q5lWuh1Y0FF2P3B+Zv4Y8KfA\nstLpucAi4LzS5paImFLarASuBuaV5cA+rwK+k5nvAH4T+MyhDFySdPiMGQ6Z+S3g5Y6y+zJzf7m7\nEZhd1hcCd2Xma5m5A9gOXBgRZwLTM3NjNj8Duwa4rNVmdVm/G7g4ImKQByVJGsxEXJD+Z8D6sj4L\n2NXatruUzSrrneVvaFMC5xXgtAkYlySpTwOFQ0R8CtgPfHFihjNmf0siYnNEbN63b9+R6FKSjkt9\nh0NEfIzmQvU/zR/8j0F7gLNa1WaXsj38YOqpXf6GNhExFTiF5sJ0JTNXZeZoZo6OjIx5sV2S1Ke+\nwiEiFgCfBD6SmX/d2rQOWBQR0yJiLs2F502ZuRd4NSLml+sJVwL3tNosLuuXAw+k/z2dJE2qQ/ko\n653ARcDpEbEbuInm00nTgPvLteONmfkvMnNrRKwFnqCZbro2M18vu7qGH3yUdT0/uE5xK3BHRGyn\nufC9aGIemiSpX3G0vkkfHR1Nv+cgSeMTEQ9n5uhY9Y7ab0i3zVn6la7lz6649AiPRJKODf62kiSp\nYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhI\nkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpMmY4RMQX\nIuLFiHi8VXZqRNwfEU+X2xmtbcsiYntEPBURl7TKL4iILWXbzRERpXxaRPxeKX8wIuZM7EOUJI3X\noZw53A4s6ChbCmzIzHnAhnKfiDgXWAScV9rcEhFTSpuVwNXAvLIc2OdVwHcy8x3AbwKf6ffBSJIm\nxpjhkJnfAl7uKF4IrC7rq4HLWuV3ZeZrmbkD2A5cGBFnAtMzc2NmJrCmo82Bfd0NXHzgrEKSNDn6\nveYwMzP3lvXngZllfRawq1VvdymbVdY7y9/QJjP3A68Ap/U5LknSBBj4gnQ5E8gJGMuYImJJRGyO\niM379u07El1K0nGp33B4oUwVUW5fLOV7gLNa9WaXsj1lvbP8DW0iYipwCvBSt04zc1Vmjmbm6MjI\nSJ9DlySNpd9wWAcsLuuLgXta5YvKJ5Dm0lx43lSmoF6NiPnlesKVHW0O7Oty4IFyNiJJmiRTx6oQ\nEXcCFwGnR8Ru4CZgBbA2Iq4CdgJXAGTm1ohYCzwB7AeuzczXy66uofnk04nA+rIA3ArcERHbaS58\nL5qQRyZJ6tuY4ZCZP9dj08U96i8Hlncp3wyc36X8/wEfHWsckqQjx29IS5IqhoMkqWI4SJIqhoMk\nqWI4SJIqhoMkqWI4SJIqhoMkqTLml+COVXOWfqVr+bMrLj3CI5Gk4eOZgySpYjhIkiqGgySpYjhI\nkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqG\ngySpYjhIkioDhUNEfCIitkbE4xFxZ0T8UEScGhH3R8TT5XZGq/6yiNgeEU9FxCWt8gsiYkvZdnNE\nxCDjkiQNpu9wiIhZwPXAaGaeD0wBFgFLgQ2ZOQ/YUO4TEeeW7ecBC4BbImJK2d1K4GpgXlkW9Dsu\nSdLgBp1WmgqcGBFTgZOAPwcWAqvL9tXAZWV9IXBXZr6WmTuA7cCFEXEmMD0zN2ZmAmtabSRJk6Dv\ncMjMPcBvAM8Be4FXMvM+YGZm7i3VngdmlvVZwK7WLnaXslllvbNckjRJBplWmkFzNjAXeBvwloj4\nhXadciaQA43wjX0uiYjNEbF53759E7VbSVKHQaaVPgTsyMx9mfl94PeB9wMvlKkiyu2Lpf4e4KxW\n+9mlbE9Z7yyvZOaqzBzNzNGRkZEBhi5JOphBwuE5YH5EnFQ+XXQxsA1YBywudRYD95T1dcCiiJgW\nEXNpLjxvKlNQr0bE/LKfK1ttJEmTYGq/DTPzwYi4G/g2sB94BFgFnAysjYirgJ3AFaX+1ohYCzxR\n6l+bma+X3V0D3A6cCKwviyRpkvQdDgCZeRNwU0fxazRnEd3qLweWdynfDJw/yFgkSRPHb0hLkiqG\ngySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySp\nYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpMlA4\nRMRbI+LuiHgyIrZFxE9GxKkRcX9EPF1uZ7TqL4uI7RHxVERc0iq/ICK2lG03R0QMMi5J0mAGPXP4\nHHBvZp4DvBvYBiwFNmTmPGBDuU9EnAssAs4DFgC3RMSUsp+VwNXAvLIsGHBckqQB9B0OEXEK8PeB\nWwEy83uZ+V1gIbC6VFsNXFbWFwJ3ZeZrmbkD2A5cGBFnAtMzc2NmJrCm1UaSNAkGOXOYC+wDbouI\nRyLi8xHxFmBmZu4tdZ4HZpb1WcCuVvvdpWxWWe8slyRNkkHCYSrwPmBlZr4X+D+UKaQDyplADtDH\nG0TEkojYHBGb9+3bN1G7lSR1GCQcdgO7M/PBcv9umrB4oUwVUW5fLNv3AGe12s8uZXvKemd5JTNX\nZeZoZo6OjIwMMHRJ0sH0HQ6Z+TywKyLeVYouBp4A1gGLS9li4J6yvg5YFBHTImIuzYXnTWUK6tWI\nmF8+pXRlq40kaRJMHbD9dcAXI+LNwDPAx2kCZ21EXAXsBK4AyMytEbGWJkD2A9dm5utlP9cAtwMn\nAuvLIkmaJAOFQ2Y+Cox22XRxj/rLgeVdyjcD5w8yFknSxPEb0pKkiuEgSaoYDpKkiuEgSaoYDpKk\niuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEg\nSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoMHA4RMSUiHomIPyz3T42I+yPi\n6XI7o1V3WURsj4inIuKSVvkFEbGlbLs5ImLQcUmS+jd1AvZxA7ANmF7uLwU2ZOaKiFha7t8YEecC\ni4DzgLcBX4+Id2bm68BK4GrgQeCrwAJg/QSMbcLMWfqVruXPrrj0CI9Ekg6/gc4cImI2cCnw+Vbx\nQmB1WV8NXNYqvyszX8vMHcB24MKIOBOYnpkbMzOBNa02kqRJMOi00n8CPgn8TatsZmbuLevPAzPL\n+ixgV6ve7lI2q6x3lkuSJknf4RARHwZezMyHe9UpZwLZbx9d+lwSEZsjYvO+ffsmareSpA6DnDl8\nAPhIRDwL3AX8dET8LvBCmSqi3L5Y6u8Bzmq1n13K9pT1zvJKZq7KzNHMHB0ZGRlg6JKkg+k7HDJz\nWWbOzsw5NBeaH8jMXwDWAYtLtcXAPWV9HbAoIqZFxFxgHrCpTEG9GhHzy6eUrmy1kSRNgon4tFKn\nFcDaiLgK2AlcAZCZWyNiLfAEsB+4tnxSCeAa4HbgRJpPKQ3VJ5Uk6XgzIeGQmd8EvlnWXwIu7lFv\nObC8S/lm4PyJGIskaXB+Q1qSVDEcJEkVw0GSVDEcJEkVw0GSVDkcH2UVvX+oD/yxPknDzzMHSVLF\ncJAkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVfwS3BDp9cU5vzQn6UjzzEGSVDEcJEkVp5WO\nYk5DSTpcDIfjzHgDxQCSjk9OK0mSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKnSdzhE\nxFkR8Y2IeCIitkbEDaX81Ii4PyKeLrczWm2WRcT2iHgqIi5plV8QEVvKtpsjIgZ7WJKkQQxy5rAf\n+JXMPBeYD1wbEecCS4ENmTkP2FDuU7YtAs4DFgC3RMSUsq+VwNXAvLIsGGBckqQB9R0Ombk3M79d\n1v8S2AbMAhYCq0u11cBlZX0hcFdmvpaZO4DtwIURcSYwPTM3ZmYCa1ptJEmTYEKuOUTEHOC9wIPA\nzMzcWzY9D8ws67OAXa1mu0vZrLLeWS5JmiQD//BeRJwMfBn4l5n5avtyQWZmROSgfbT6WgIsAXj7\n298+UbvVBOr1Q33gj/VJR5OBzhwi4k00wfDFzPz9UvxCmSqi3L5YyvcAZ7Wazy5le8p6Z3klM1dl\n5mhmjo6MjAwydEnSQQzyaaUAbgW2ZeZnW5vWAYvL+mLgnlb5ooiYFhFzaS48bypTUK9GxPyyzytb\nbSRJk2CQaaUPAL8IbImIR0vZvwJWAGsj4ipgJ3AFQGZujYi1wBM0n3S6NjNfL+2uAW4HTgTWl0WS\nNEn6DofM/COg1/cRLu7RZjmwvEv5ZuD8fsciSZpYfkNaklQxHCRJFcNBklQZ+HsO0qB6fTfC70VI\nk8czB0lSxXCQJFWcVtJRx2ko6fDzzEGSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkVw0GSVDEcJEkV\nw0GSVPEb0joujPdb1X4LW8c7zxwkSRXDQZJUcVpJmgC9pqFg4qaunOrSkWQ4SMcwA0X9clpJklQx\nHCRJFaeVJP2tiboO0k+bibzW4vWcwXnmIEmqGA6SpMrQhENELIiIpyJie0QsnezxSNLxbCiuOUTE\nFOC3gX8A7AYeioh1mfnE5I5MkmrHyrWWgxmWM4cLge2Z+Uxmfg+4C1g4yWOSpOPWsITDLGBX6/7u\nUiZJmgSRmZM9BiLicmBBZv7zcv8XgZ/IzF/qqLcEWFLuvgt4qsvuTgf+YpxDGG+bw13/WOljGMd0\nJPoYxjEdiT6GcUxHoo9hHNPB2pydmSNjts7MSV+AnwS+1rq/DFjW5742H+42h7v+sdLHMI7Jxz08\n9Y+VPoZxTP22aS/DMq30EDAvIuZGxJuBRcC6SR6TJB23huLTSpm5PyJ+CfgaMAX4QmZuneRhSdJx\nayjCASAzvwp8dQJ2teoItDnc9Y+VPoZxTEeij2Ec05HoYxjHdCT6GMYx9dvmbw3FBWlJ0nAZlmsO\nkqQhYjhIkiqGgySpMjQXpPsVEefQ/NTGgW9U7wHWZea2Ce5jFvBgZv5Vq3xBZt7bpf6FQGbmQxFx\nLrAAeLJcdD+U/tZk5pXjGN9P0fwEyeOZeV+X7T8BbMvMVyPiRGAp8D7gCeDXM/OVLm2uB/5bZu7q\n3NZjDAc+gvznmfn1iPh54P3ANmBVZn6/R7u/A/wscBbwOvCnwJcy89VD6Vc6GkXEj2Tmi4dx/6dl\n5kuD7OOoPnOIiBtpfocpgE1lCeDOfn7ZNSI+3qXseuAe4Drg8Yho/+bTr3epfxNwM7AyIj4N/Bbw\nFmBpRHyqS/11Hct/B372wP0e49zUWr+69PHDwE09HvcXgL8u658DTgE+U8pu69YH8O+BByPif0bE\nNREx1jcqbwMuBW6IiDuAjwIPAj8OfL7H47ge+M/AD5V602hCYmNEXDRGf8eUiPiRI9DHaYe7j4kU\nEadExIqIeDIiXo6IlyJiWyl76zj3tb5H+fSI+HRE3FHe0LS33dKl/hkRsTIifjsiTouIX42ILRGx\nNiLO7NHHqR3LacCmiJgREad2qb+g4zm4NSIei4gvRcTMLvVXRMTpZX00Ip6h+dvdGREfHOu56WmQ\nb9BN9kLzLvNNXcrfDDzdx/6e61K2BTi5rM8BNgM3lPuP9Kg/BTgJeBWYXspPBB7rUv/bwO8CFwEf\nLLd7y/oHe4zzkdb6Q8BIWX8LsKVL/W3t/jq2PdqrD5o3D/8QuBXYB9wLLAZ+uEv9x8rtVOAFYEq5\nH90ed/u5KusnAd8s62/v8dyeAqwAngReBl6iOTNZAby1j3/v9V3KpgOfBu4Afr5j2y099nMGsJLm\nl4VPA361PLa1wJld6p/asZwGPAvMAE7tUn9Bx3NwK/AY8CVgZo8xrQBOL+ujwDPAdmBnt9dVeR3+\na+DvjuP5GwW+UV6/ZwH3A6+U1+R7u9Q/Gfh3wNZSbx+wEfhYj/1/DbgROKPjub4RuK9L/ff1WC4A\n9vbo48vlubqM5ou3XwamdftbKWX30rxRXFr+DW4sj/064J4effwNsKNj+X65fabbv0Vr/fPArwFn\nA58A/qDb31Fr/RvAj5f1dzLAt6T7ajQsC81B4uwu5WcDT/Vo81iPZQvwWpf6W7u8wO8FPkuXAytv\nPHA/0rGtW/0Tyj/6/cB7Sln1gulo8yc0B5LTOl/AnX2Wsv8KfLys3waMtl48D/Xoo3O/bwI+AtwJ\n7OtS/3GaUJ4B/CXlIEdzVrCtRx9bWn+IM9ovZJopss764zpYlO3jOmAwzoNFKR/XAYPDfLA48Ny2\n1sc8YJS+fwN4juYM/BPA28Z4HW4Cfgb4OZofzry8lF8M/HGX+vcAHwNmA78M/BtgHrCaZnqzs37X\nv+Fe22imJR8oj7dz+b899vNox/1PAf+LLn9bXf6+nzvYvlrlv1JeIz/afr4P8ti+fZDxdTuGbAOm\nlvWNvV4H4136ajQsC81c/nZgPc0XPlaVf4TttN5tdbR5AXhP+eNqL3No5ss76z9AOWi3yqYCa4DX\nu9R/EDiprJ/QKj+l24uttX02zUH8tzpfdF3qPkvzTnBHuT2zlJ/c48VzCnA78GdlfN8v7f4H8O4e\nfVQh09p2UpeyT5R97gSuBzYAv0MTADf12M8NNAfT36EJ+gMBNgJ8q0v9cR0sSvm4DhjjPVh0PleH\ncsA43AeLUj6uA0ZHH38PuAV4vjxPS/p43N3epPxJx/2HDvyd0FyT66x/H/BJWmdHwEya8P16l/qP\nA/N6jHXXQZ6nEzrKPkZzdrPzYI8B+LWxntfWtgN/35+lmQLu+QaQ5lepf7m8TnZQvo9WtnWbfbiu\nPFc/TXPW+jmamYd/C9zRq5+xlr4aDdNSXljzgX9clvmUqYoe9W8FfqrHti/1+Ec9o0f9D3Qpm9aj\n7untg8FBxncpXd5FHeJzcRIw9yDbpwPvpnnX3HU6olX3nX30/zbKu03grcDlwIVjtDmv1DvnEPY/\nroNF2T6uA8Z4DxZl+7gPGIfzYFHKx3XAoPu75Ck0b8Bu69HHH9NMO36U5k3BZaX8g3Q/O/nfB/72\naM5C2z+22e1MYAbNtbEnge/QTCVuK2Xdpt8uB97VY6yX9Sj/D8CHupQvoMvUNM202Mldyt8B3H0I\nr+GP0EylPX+QOjd1LAemjc8A1vRocxHwezTTwVtofm1iCV2m3Q916auRi8tkLB0Hi5c7DhYzerQZ\n1wFjvAeLsq3vA8bhOliU7b0OGFO71L2rj3+Pd9NM9a0HzqEJoO/SBOn7u9T/MZqpqO8Af0R5A0Jz\npnh9jz7OAT7U+fzSe2bgHJpprUOqP0abnzkcfdBcfzx/Ih9HP2Ma89+334YuLsO0UKakDmebw9VH\nx8FiKMY0DH3QTE8+BfwBzVTqwta2bmc646pfyq8bZx/jqt/n4xjvmMb9uA/p36Tfhi4uw7QwxnWa\niWgzjH0M45gmqg/6+6TgIdc/VvroZ0yHshz1X4LT8SMiHuu1iebaw8BthrGPYRzTEerjhCxfOs3M\nZ8t3X+6OiLNLm0HrHyt99DOmMRkOOprMBC6hmbNuC5qLnRPRZhj7GMYxHYk+XoiI92TmowCZ+VcR\n8WGaL3X+6ATUP1b66GdMYzIcdDT5Q5rT50c7N0TENyeozTD2MYxjOhJ9XAnsbxdk5n7gyoj4LxNQ\n/1jpo58xjcn/z0GSVDmqf1tJknR4GA6SpIrhIEmqGA6SpIrhIEmq/H+TKcnkaDwSogAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1205fde80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Hid_vec.hand.value_counts().plot.bar(use_index=False)\n",
    "# Don't show labels because of NDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, only use items with at least 1k, and cut everything off to 1k. Randomly select which 1k to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handover1k = all_withhand.hand.value_counts() > 1000\n",
    "handover1k = handover1k[handover1k]\n",
    "handover1k = handover1k.index\n",
    "len(handover1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allhand = pd.DataFrame()\n",
    "for cat in handover1k:\n",
    "    mycat = all_withhand[all_withhand.hand==cat]\n",
    "    mycat = mycat.sample(n=1000, random_state=0)\n",
    "    allhand = allhand.append(mycat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce dimensionality in image vector\n",
    "### (sparse 4k matrix --> dense 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=300, n_iter=7, random_state=0)\n",
    "allhand_id = allhand[allhand.columns[0:4]]\n",
    "allhand_text = allhand[allhand.columns[4:304]]\n",
    "allhand_imageFULL = allhand[allhand.columns[304:4399]]\n",
    "\n",
    "allhand_image = svd.fit_transform(allhand_imageFULL)\n",
    "allhand_image = pd.DataFrame(allhand_image)\n",
    "allhand_image.index = allhand_imageFULL.index\n",
    "\n",
    "allhand_reduced = pd.concat([allhand_id, allhand_text, allhand_image], axis=1)\n",
    "# allhand_reduced.to_csv('handlabeled_vectors.csv')\n",
    "allhand = allhand_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check feature variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12dcd4358>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1266bd748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log(np.std(allhand_text, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12d6fcb38>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC1TEnNTxjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fff1240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log(np.std(allhand_image, axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classification\n",
    "### How easy is it to classify into their hand-labeled categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From text alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allhand_text\n",
    "y = allhand_id.hand\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7.652992010116577 seconds.\n",
      "Accuracty: 0.845090909091\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.96      0.97      0.96       237\n",
      "      coats       0.85      0.86      0.85       270\n",
      "    dresses       0.75      0.86      0.80       240\n",
      "    jackets       0.82      0.76      0.79       250\n",
      "     shirts       0.76      0.76      0.76       227\n",
      "      shoes       0.97      0.96      0.96       253\n",
      "     shorts       0.76      0.76      0.76       251\n",
      "     skirts       0.89      0.85      0.87       266\n",
      "   swimwear       0.88      0.87      0.88       263\n",
      "   trousers       0.78      0.86      0.82       244\n",
      "  underwear       0.88      0.78      0.83       249\n",
      "\n",
      "avg / total       0.85      0.85      0.85      2750\n",
      "\n",
      "[[230   0   0   2   0   0   0   0   5   0   0]\n",
      " [  0 232   3  21   5   0   1   2   2   3   1]\n",
      " [  0   1 206   3  10   1   5   6   2   3   3]\n",
      " [  1  27   6 191  12   0   5   1   3   3   1]\n",
      " [  1   5  15   6 172   1   6   5   1  11   4]\n",
      " [  2   0   2   1   2 242   1   0   1   0   2]\n",
      " [  0   4  12   1   4   0 192   4   7  27   0]\n",
      " [  0   0   8   2   4   1  16 225   3   4   3]\n",
      " [  2   0   5   0   2   2   9   3 229   2   9]\n",
      " [  2   4   2   3   2   0  12   5   1 210   3]\n",
      " [  2   0  16   2  12   2   6   2   5   7 195]]\n"
     ]
    }
   ],
   "source": [
    "# Try logistic regression\n",
    "tic = time.time()\n",
    "lr = LogisticRegression(random_state=1)\n",
    "lr.fit(X, y)\n",
    "lr_y_pred = lr.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, lr_y_pred)))\n",
    "print(classification_report(y_test, lr_y_pred))\n",
    "print(confusion_matrix(y_test, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4.889408111572266 seconds.\n",
      "Accuracty: 0.847272727273\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.96      0.96      0.96       237\n",
      "      coats       0.86      0.84      0.85       270\n",
      "    dresses       0.78      0.83      0.81       240\n",
      "    jackets       0.82      0.80      0.81       250\n",
      "     shirts       0.76      0.76      0.76       227\n",
      "      shoes       0.95      0.95      0.95       253\n",
      "     shorts       0.76      0.79      0.77       251\n",
      "     skirts       0.88      0.84      0.86       266\n",
      "   swimwear       0.90      0.90      0.90       263\n",
      "   trousers       0.82      0.86      0.84       244\n",
      "  underwear       0.83      0.80      0.81       249\n",
      "\n",
      "avg / total       0.85      0.85      0.85      2750\n",
      "\n",
      "[[228   0   0   2   1   0   0   1   4   0   1]\n",
      " [  0 226   2  27   5   1   2   2   2   2   1]\n",
      " [  0   1 199   3  13   2   6   8   1   1   6]\n",
      " [  2  25   3 199   9   0   2   3   1   5   1]\n",
      " [  1   3  15   5 172   1   4   6   1  11   8]\n",
      " [  2   0   1   1   0 241   3   0   2   0   3]\n",
      " [  1   1   7   1   3   2 198   5   8  19   6]\n",
      " [  0   1  11   2   2   3  14 224   3   2   4]\n",
      " [  0   1   5   0   3   2   7   1 236   1   7]\n",
      " [  2   3   1   2   3   0  17   3   0 209   4]\n",
      " [  2   3  10   1  16   1   9   1   4   4 198]]\n"
     ]
    }
   ],
   "source": [
    "# Try linear SVM\n",
    "tic = time.time()\n",
    "svc = LinearSVC(random_state=1)\n",
    "svc.fit(X_train, y_train)\n",
    "svc_y_pred = svc.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, svc_y_pred)))\n",
    "print(classification_report(y_test, svc_y_pred))\n",
    "print(confusion_matrix(y_test, svc_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.24440407752990723 seconds.\n",
      "Accuracty: 0.527272727273\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.95      0.74      0.83       237\n",
      "      coats       0.69      0.37      0.48       270\n",
      "    dresses       0.53      0.74      0.62       240\n",
      "    jackets       0.40      0.58      0.47       250\n",
      "     shirts       0.61      0.32      0.42       227\n",
      "      shoes       0.96      0.72      0.82       253\n",
      "     shorts       0.49      0.13      0.21       251\n",
      "     skirts       0.76      0.48      0.59       266\n",
      "   swimwear       0.37      0.78      0.50       263\n",
      "   trousers       0.48      0.68      0.56       244\n",
      "  underwear       0.24      0.25      0.24       249\n",
      "\n",
      "avg / total       0.59      0.53      0.52      2750\n",
      "\n",
      "[[175   0   3  18   0   4   0   0  24   1  12]\n",
      " [  1 100   8 120   6   0   2   0   8   6  19]\n",
      " [  0   2 178   7  11   0   0   6  13   5  18]\n",
      " [  5  27   9 146   8   1   2   3  28   7  14]\n",
      " [  0   5  29  35  72   0   1   5  33  26  21]\n",
      " [  0   3  12   2   0 182   0   0  28   1  25]\n",
      " [  1   3  22  11   8   1  33   6  53  83  30]\n",
      " [  0   1  37   7   2   0   7 129  47  17  19]\n",
      " [  0   0   8   1   1   1   7   3 206   8  28]\n",
      " [  1   4   4  18   6   0   7   4  17 166  17]\n",
      " [  1   0  25   4   4   1   8  14 100  29  63]]\n"
     ]
    }
   ],
   "source": [
    "# Try naive bayes\n",
    "tic = time.time()\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_y_pred = gnb.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, gnb_y_pred)))\n",
    "print(classification_report(y_test, gnb_y_pred))\n",
    "print(confusion_matrix(y_test, gnb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18.598824977874756 seconds.\n",
      "Accuracty: 0.723636363636\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.88      0.92      0.90       237\n",
      "      coats       0.69      0.81      0.74       270\n",
      "    dresses       0.57      0.78      0.66       240\n",
      "    jackets       0.56      0.67      0.61       250\n",
      "     shirts       0.77      0.46      0.57       227\n",
      "      shoes       0.97      0.82      0.89       253\n",
      "     shorts       0.66      0.69      0.67       251\n",
      "     skirts       0.78      0.62      0.69       266\n",
      "   swimwear       0.75      0.86      0.80       263\n",
      "   trousers       0.70      0.73      0.71       244\n",
      "  underwear       0.82      0.59      0.68       249\n",
      "\n",
      "avg / total       0.74      0.72      0.72      2750\n",
      "\n",
      "[[219   0   1   2   2   3   1   2   4   0   3]\n",
      " [  5 220   3  26   7   0   0   2   1   5   1]\n",
      " [  0   8 187  13   6   1   7   8   4   5   1]\n",
      " [  3  50   7 167   1   1   8   3   4   6   0]\n",
      " [  3  19  32  28 104   1   9   4   5  14   8]\n",
      " [  7   3   5  11   1 207   3   2   7   3   4]\n",
      " [  3   5  14  11   2   0 173  12   7  24   0]\n",
      " [  1   4  46  13   2   0  22 165   5   6   2]\n",
      " [  3   0   5   3   2   1   5   3 225   6  10]\n",
      " [  2  10   6  13   2   0  25   4   2 177   3]\n",
      " [  2   2  20  12   6   0   9   6  38   8 146]]\n"
     ]
    }
   ],
   "source": [
    "# Try KNN\n",
    "tic = time.time()\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_y_pred = knn.predict(X_test)\n",
    "toc = time.time()\n",
    "\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, knn_y_pred)))\n",
    "print(classification_report(y_test, knn_y_pred))\n",
    "print(confusion_matrix(y_test, knn_y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5.286087989807129 seconds.\n",
      "Accuracty: 0.476727272727\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.69      0.68      0.68       237\n",
      "      coats       0.57      0.50      0.53       270\n",
      "    dresses       0.47      0.49      0.48       240\n",
      "    jackets       0.39      0.40      0.40       250\n",
      "     shirts       0.28      0.29      0.29       227\n",
      "      shoes       0.62      0.58      0.60       253\n",
      "     shorts       0.44      0.49      0.46       251\n",
      "     skirts       0.47      0.46      0.46       266\n",
      "   swimwear       0.52      0.50      0.51       263\n",
      "   trousers       0.40      0.41      0.40       244\n",
      "  underwear       0.43      0.43      0.43       249\n",
      "\n",
      "avg / total       0.48      0.48      0.48      2750\n",
      "\n",
      "[[160   8   6   6   6  18   6   6   9   1  11]\n",
      " [  7 136   9  44  21  11   6   7  11  12   6]\n",
      " [  4   6 118  10  22   7  11  20  11  12  19]\n",
      " [  9  33  12 101  25   5  18  15   7  20   5]\n",
      " [  6  13  16  18  66  10  19  18  11  31  19]\n",
      " [ 18   9  10  10   8 146   5   8  13  10  16]\n",
      " [  6   6  16  10  12   5 124  24  12  22  14]\n",
      " [  5  10  22  15  22   9  26 122  12   8  15]\n",
      " [  6   5  16  16  10   7  20  17 131  14  21]\n",
      " [  4  11  10  12  22  10  35  11  12  99  18]\n",
      " [  6   3  17  16  20   9  13  13  25  19 108]]\n"
     ]
    }
   ],
   "source": [
    "# Try decision tree, maybe random forest later\n",
    "tic = time.time()\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_y_pred = dt.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, dt_y_pred)))\n",
    "print(classification_report(y_test, dt_y_pred))\n",
    "print(confusion_matrix(y_test, dt_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8250 samples, validate on 2750 samples\n",
      "Epoch 1/100\n",
      "8250/8250 [==============================] - 1s - loss: 2.0036 - acc: 0.4864 - val_loss: 1.5529 - val_acc: 0.6491\n",
      "Epoch 2/100\n",
      "8250/8250 [==============================] - 0s - loss: 1.2848 - acc: 0.6762 - val_loss: 1.0972 - val_acc: 0.7102\n",
      "Epoch 3/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.9847 - acc: 0.7372 - val_loss: 0.9054 - val_acc: 0.7589\n",
      "Epoch 4/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.8352 - acc: 0.7714 - val_loss: 0.8029 - val_acc: 0.7789\n",
      "Epoch 5/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.7462 - acc: 0.7919 - val_loss: 0.7383 - val_acc: 0.7949\n",
      "Epoch 6/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.6841 - acc: 0.8055 - val_loss: 0.6945 - val_acc: 0.8029\n",
      "Epoch 7/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.6367 - acc: 0.8159 - val_loss: 0.6668 - val_acc: 0.8098\n",
      "Epoch 8/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.5997 - acc: 0.8316 - val_loss: 0.6361 - val_acc: 0.8182\n",
      "Epoch 9/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.5707 - acc: 0.8342 - val_loss: 0.6243 - val_acc: 0.8240\n",
      "Epoch 10/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.5430 - acc: 0.8424 - val_loss: 0.6058 - val_acc: 0.8265\n",
      "Epoch 11/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.5215 - acc: 0.8487 - val_loss: 0.5978 - val_acc: 0.8258\n",
      "Epoch 12/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.5021 - acc: 0.8554 - val_loss: 0.5950 - val_acc: 0.8269\n",
      "Epoch 13/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.4850 - acc: 0.8589 - val_loss: 0.5848 - val_acc: 0.8324\n",
      "Epoch 14/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.4689 - acc: 0.8633 - val_loss: 0.5756 - val_acc: 0.8371\n",
      "Epoch 15/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.4568 - acc: 0.8667 - val_loss: 0.5740 - val_acc: 0.8349\n",
      "Epoch 16/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.4448 - acc: 0.8688 - val_loss: 0.5669 - val_acc: 0.8353\n",
      "Epoch 17/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.4336 - acc: 0.8732 - val_loss: 0.5600 - val_acc: 0.8396\n",
      "Epoch 18/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.4227 - acc: 0.8748 - val_loss: 0.5568 - val_acc: 0.8407\n",
      "Epoch 19/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.4136 - acc: 0.8782 - val_loss: 0.5558 - val_acc: 0.8411\n",
      "Epoch 20/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.4042 - acc: 0.8792 - val_loss: 0.5489 - val_acc: 0.8433\n",
      "Epoch 21/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3958 - acc: 0.8827 - val_loss: 0.5526 - val_acc: 0.8422\n",
      "Epoch 22/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3891 - acc: 0.8863 - val_loss: 0.5479 - val_acc: 0.8425\n",
      "Epoch 23/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3812 - acc: 0.8862 - val_loss: 0.5616 - val_acc: 0.8491\n",
      "Epoch 24/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3736 - acc: 0.8872 - val_loss: 0.5472 - val_acc: 0.8447\n",
      "Epoch 25/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3669 - acc: 0.8902 - val_loss: 0.5480 - val_acc: 0.8484\n",
      "Epoch 26/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3588 - acc: 0.8956 - val_loss: 0.5514 - val_acc: 0.8433\n",
      "Epoch 27/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3542 - acc: 0.8950 - val_loss: 0.5523 - val_acc: 0.8429\n",
      "Epoch 28/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3482 - acc: 0.8942 - val_loss: 0.5478 - val_acc: 0.8440\n",
      "Epoch 29/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3420 - acc: 0.8979 - val_loss: 0.5450 - val_acc: 0.8487\n",
      "Epoch 30/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3356 - acc: 0.9021 - val_loss: 0.5444 - val_acc: 0.8465\n",
      "Epoch 31/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3303 - acc: 0.9030 - val_loss: 0.5488 - val_acc: 0.8480\n",
      "Epoch 32/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3276 - acc: 0.9011 - val_loss: 0.5477 - val_acc: 0.8487\n",
      "Epoch 33/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3189 - acc: 0.9070 - val_loss: 0.5421 - val_acc: 0.8473\n",
      "Epoch 34/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3156 - acc: 0.9065 - val_loss: 0.5493 - val_acc: 0.8447\n",
      "Epoch 35/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3108 - acc: 0.9076 - val_loss: 0.5494 - val_acc: 0.8440\n",
      "Epoch 36/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3077 - acc: 0.9098 - val_loss: 0.5487 - val_acc: 0.8469\n",
      "Epoch 37/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3022 - acc: 0.9116 - val_loss: 0.5509 - val_acc: 0.8447\n",
      "Epoch 38/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2989 - acc: 0.9130 - val_loss: 0.5470 - val_acc: 0.8495\n",
      "Epoch 39/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2921 - acc: 0.9147 - val_loss: 0.5486 - val_acc: 0.8491\n",
      "Epoch 40/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2882 - acc: 0.9160 - val_loss: 0.5451 - val_acc: 0.8513\n",
      "Epoch 41/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2866 - acc: 0.9165 - val_loss: 0.5610 - val_acc: 0.8495\n",
      "Epoch 42/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2811 - acc: 0.9194 - val_loss: 0.5540 - val_acc: 0.8480\n",
      "Epoch 43/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2759 - acc: 0.9225 - val_loss: 0.5484 - val_acc: 0.8524\n",
      "Epoch 44/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2732 - acc: 0.9221 - val_loss: 0.5547 - val_acc: 0.8498\n",
      "Epoch 45/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2701 - acc: 0.9201 - val_loss: 0.5539 - val_acc: 0.8491\n",
      "Epoch 46/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2664 - acc: 0.9216 - val_loss: 0.5780 - val_acc: 0.8415\n",
      "Epoch 47/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2627 - acc: 0.9245 - val_loss: 0.5624 - val_acc: 0.8502\n",
      "Epoch 48/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2578 - acc: 0.9278 - val_loss: 0.5617 - val_acc: 0.8487\n",
      "Epoch 49/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2565 - acc: 0.9248 - val_loss: 0.5725 - val_acc: 0.8495\n",
      "Epoch 50/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2516 - acc: 0.9284 - val_loss: 0.5586 - val_acc: 0.8484\n",
      "Epoch 51/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2489 - acc: 0.9305 - val_loss: 0.5586 - val_acc: 0.8473\n",
      "Epoch 52/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2438 - acc: 0.9324 - val_loss: 0.5719 - val_acc: 0.8498\n",
      "Epoch 53/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2414 - acc: 0.9342 - val_loss: 0.5637 - val_acc: 0.8516\n",
      "Epoch 54/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2383 - acc: 0.9327 - val_loss: 0.5695 - val_acc: 0.8516\n",
      "Epoch 55/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2344 - acc: 0.9347 - val_loss: 0.5660 - val_acc: 0.8484\n",
      "Epoch 56/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2325 - acc: 0.9344 - val_loss: 0.5700 - val_acc: 0.8484\n",
      "Epoch 57/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2298 - acc: 0.9365 - val_loss: 0.5727 - val_acc: 0.8516\n",
      "Epoch 58/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2283 - acc: 0.9359 - val_loss: 0.5820 - val_acc: 0.8502\n",
      "Epoch 59/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2231 - acc: 0.9390 - val_loss: 0.5711 - val_acc: 0.8480\n",
      "Epoch 60/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2217 - acc: 0.9385 - val_loss: 0.5740 - val_acc: 0.8542\n",
      "Epoch 61/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2168 - acc: 0.9405 - val_loss: 0.5800 - val_acc: 0.8491\n",
      "Epoch 62/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2137 - acc: 0.9432 - val_loss: 0.5769 - val_acc: 0.8509\n",
      "Epoch 63/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2127 - acc: 0.9401 - val_loss: 0.5790 - val_acc: 0.8487\n",
      "Epoch 64/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2122 - acc: 0.9438 - val_loss: 0.5841 - val_acc: 0.8527\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8250/8250 [==============================] - 0s - loss: 0.2061 - acc: 0.9434 - val_loss: 0.5829 - val_acc: 0.8484\n",
      "Epoch 66/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2049 - acc: 0.9433 - val_loss: 0.5893 - val_acc: 0.8553\n",
      "Epoch 67/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2002 - acc: 0.9468 - val_loss: 0.5910 - val_acc: 0.8476\n",
      "Epoch 68/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1999 - acc: 0.9450 - val_loss: 0.5916 - val_acc: 0.8473\n",
      "Epoch 69/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1957 - acc: 0.9474 - val_loss: 0.5914 - val_acc: 0.8513\n",
      "Epoch 70/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1921 - acc: 0.9498 - val_loss: 0.5980 - val_acc: 0.8538\n",
      "Epoch 71/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1914 - acc: 0.9472 - val_loss: 0.5974 - val_acc: 0.8549\n",
      "Epoch 72/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1901 - acc: 0.9468 - val_loss: 0.6002 - val_acc: 0.8505\n",
      "Epoch 73/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1888 - acc: 0.9490 - val_loss: 0.5957 - val_acc: 0.8513\n",
      "Epoch 74/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1848 - acc: 0.9510 - val_loss: 0.5961 - val_acc: 0.8520\n",
      "Epoch 75/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1809 - acc: 0.9538 - val_loss: 0.6044 - val_acc: 0.8491\n",
      "Epoch 76/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1827 - acc: 0.9496 - val_loss: 0.6096 - val_acc: 0.8520\n",
      "Epoch 77/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.1784 - acc: 0.9536 - val_loss: 0.6113 - val_acc: 0.8516\n",
      "Epoch 78/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1761 - acc: 0.9531 - val_loss: 0.6060 - val_acc: 0.8495\n",
      "Epoch 79/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1712 - acc: 0.9531 - val_loss: 0.6159 - val_acc: 0.8484\n",
      "Epoch 80/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1721 - acc: 0.9532 - val_loss: 0.6141 - val_acc: 0.8502\n",
      "Epoch 81/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1693 - acc: 0.9552 - val_loss: 0.6126 - val_acc: 0.8520\n",
      "Epoch 82/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1657 - acc: 0.9560 - val_loss: 0.6053 - val_acc: 0.8527\n",
      "Epoch 83/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1623 - acc: 0.9581 - val_loss: 0.6111 - val_acc: 0.8524\n",
      "Epoch 84/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1612 - acc: 0.9585 - val_loss: 0.6195 - val_acc: 0.8556\n",
      "Epoch 85/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1594 - acc: 0.9585 - val_loss: 0.6160 - val_acc: 0.8487\n",
      "Epoch 86/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1578 - acc: 0.9590 - val_loss: 0.6203 - val_acc: 0.8513\n",
      "Epoch 87/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1554 - acc: 0.9598 - val_loss: 0.6136 - val_acc: 0.8542\n",
      "Epoch 88/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1544 - acc: 0.9578 - val_loss: 0.6234 - val_acc: 0.8513\n",
      "Epoch 89/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1526 - acc: 0.9618 - val_loss: 0.6269 - val_acc: 0.8564\n",
      "Epoch 90/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1485 - acc: 0.9617 - val_loss: 0.6245 - val_acc: 0.8509\n",
      "Epoch 91/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1473 - acc: 0.9606 - val_loss: 0.6270 - val_acc: 0.8509\n",
      "Epoch 92/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1448 - acc: 0.9625 - val_loss: 0.6313 - val_acc: 0.8545\n",
      "Epoch 93/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1426 - acc: 0.9630 - val_loss: 0.6464 - val_acc: 0.8476\n",
      "Epoch 94/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1426 - acc: 0.9629 - val_loss: 0.6325 - val_acc: 0.8513\n",
      "Epoch 95/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1424 - acc: 0.9617 - val_loss: 0.6347 - val_acc: 0.8527\n",
      "Epoch 96/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1378 - acc: 0.9659 - val_loss: 0.6450 - val_acc: 0.8516\n",
      "Epoch 97/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1363 - acc: 0.9652 - val_loss: 0.6342 - val_acc: 0.8516\n",
      "Epoch 98/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1336 - acc: 0.9679 - val_loss: 0.6530 - val_acc: 0.8451\n",
      "Epoch 99/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1311 - acc: 0.9682 - val_loss: 0.6499 - val_acc: 0.8473\n",
      "Epoch 100/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1307 - acc: 0.9659 - val_loss: 0.6409 - val_acc: 0.8531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127854a90>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try neural network\n",
    "\n",
    "# Reshape data\n",
    "# Data to numpy arrays\n",
    "npy_X_test = X_test.values\n",
    "npy_X_train = X_train.values\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "onehot_y_train = np_utils.to_categorical(encoded_y_train)\n",
    "onehot_y_test = np_utils.to_categorical(encoded_y_test)\n",
    "\n",
    "# Build model\n",
    "nn = Sequential()\n",
    "nn.add(Dense(100, input_dim=300, activation='relu'))\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train & Test\n",
    "nn.fit(npy_X_train, onehot_y_train,\n",
    "       validation_data=(npy_X_test, onehot_y_test),\n",
    "       epochs=100, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From image alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = allhand_image\n",
    "y = allhand_id.hand\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 41.384730100631714 seconds.\n",
      "Accuracty: 0.840727272727\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       1.00      1.00      1.00       237\n",
      "      coats       0.79      0.84      0.81       270\n",
      "    dresses       0.76      0.85      0.80       240\n",
      "    jackets       0.78      0.77      0.78       250\n",
      "     shirts       0.76      0.70      0.73       227\n",
      "      shoes       1.00      1.00      1.00       253\n",
      "     shorts       0.76      0.80      0.78       251\n",
      "     skirts       0.84      0.82      0.83       266\n",
      "   swimwear       0.85      0.77      0.81       263\n",
      "   trousers       0.87      0.93      0.90       244\n",
      "  underwear       0.84      0.78      0.81       249\n",
      "\n",
      "avg / total       0.84      0.84      0.84      2750\n",
      "\n",
      "[[236   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0 226   6  28   5   0   0   4   0   1   0]\n",
      " [  0   5 204   0   6   0  12   4   4   3   2]\n",
      " [  0  37   1 193  16   0   2   0   0   1   0]\n",
      " [  0   8   7  20 159   0   6   1   6  17   3]\n",
      " [  0   0   0   0   0 253   0   0   0   0   0]\n",
      " [  0   0   8   2   4   0 201  20   7   4   5]\n",
      " [  0   2  19   1   2   0  17 217   1   5   2]\n",
      " [  0   1   9   1   5   0  16   4 203   0  24]\n",
      " [  0   3   3   1   4   0   1   5   1 226   0]\n",
      " [  0   4  13   2   7   0   8   2  16   3 194]]\n"
     ]
    }
   ],
   "source": [
    "# Try logistic regression\n",
    "tic = time.time()\n",
    "lr = LogisticRegression(random_state=1)\n",
    "lr.fit(X, y)\n",
    "lr_y_pred = lr.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, lr_y_pred)))\n",
    "print(classification_report(y_test, lr_y_pred))\n",
    "print(confusion_matrix(y_test, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 36.4244019985199 seconds.\n",
      "Accuracty: 0.729818181818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.96      0.95      0.96       237\n",
      "      coats       0.65      0.79      0.71       270\n",
      "    dresses       0.69      0.64      0.67       240\n",
      "    jackets       0.75      0.46      0.57       250\n",
      "     shirts       0.59      0.59      0.59       227\n",
      "      shoes       0.95      0.97      0.96       253\n",
      "     shorts       0.68      0.71      0.69       251\n",
      "     skirts       0.55      0.78      0.65       266\n",
      "   swimwear       0.76      0.68      0.72       263\n",
      "   trousers       0.77      0.87      0.82       244\n",
      "  underwear       0.80      0.59      0.68       249\n",
      "\n",
      "avg / total       0.74      0.73      0.73      2750\n",
      "\n",
      "[[225   2   0   1   2   2   0   2   2   0   1]\n",
      " [  2 213   6  19  18   1   0   7   0   3   1]\n",
      " [  1   6 154   2   9   1   9  40   9   6   3]\n",
      " [  0  77   1 114  41   0   8   5   0   4   0]\n",
      " [  1  20   6  11 133   0  10  13   3  21   9]\n",
      " [  0   0   3   0   0 245   0   2   2   0   1]\n",
      " [  0   2   7   0   3   0 178  47   4   9   1]\n",
      " [  1   1  17   1   6   2  16 208   2  11   1]\n",
      " [  3   0  13   0   2   2  23  22 178   1  19]\n",
      " [  0   3   7   0   4   0   2  15   1 212   0]\n",
      " [  1   4   9   3   7   4  17  17  32   8 147]]\n"
     ]
    }
   ],
   "source": [
    "# Try linear SVM\n",
    "tic = time.time()\n",
    "svc = LinearSVC(random_state=1)\n",
    "svc.fit(X_train, y_train)\n",
    "svc_y_pred = svc.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, svc_y_pred)))\n",
    "print(classification_report(y_test, svc_y_pred))\n",
    "print(confusion_matrix(y_test, svc_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.3433060646057129 seconds.\n",
      "Accuracty: 0.604727272727\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.68      0.84      0.75       237\n",
      "      coats       0.66      0.50      0.57       270\n",
      "    dresses       0.66      0.51      0.58       240\n",
      "    jackets       0.56      0.61      0.59       250\n",
      "     shirts       0.67      0.48      0.56       227\n",
      "      shoes       0.85      0.84      0.85       253\n",
      "     shorts       0.38      0.80      0.52       251\n",
      "     skirts       0.83      0.52      0.64       266\n",
      "   swimwear       0.71      0.46      0.56       263\n",
      "   trousers       0.79      0.71      0.75       244\n",
      "  underwear       0.33      0.41      0.36       249\n",
      "\n",
      "avg / total       0.65      0.60      0.61      2750\n",
      "\n",
      "[[198   0   0   1   5   5   5   4   1   0  18]\n",
      " [ 12 134   2  87   4   2   8   0   0   6  15]\n",
      " [ 13   4 122   2   3   5  56   4   1   6  24]\n",
      " [  4  42   0 153  18   0  18   2   0   4   9]\n",
      " [  9  12   8  20 109   1  34   2   3  15  14]\n",
      " [  3   3   0   0   9 212   3   1   3   0  19]\n",
      " [  2   2   4   0   1   1 202   7   2   5  25]\n",
      " [ 12   2  21   1   1   1  64 137   0   7  20]\n",
      " [ 14   0   9   0   5   6  73   0 121   0  35]\n",
      " [  4   3   7   4   4   5  10   6   1 174  26]\n",
      " [ 20   2  11   3   4  10  54   2  39   3 101]]\n"
     ]
    }
   ],
   "source": [
    "# Try naive bayes\n",
    "tic = time.time()\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_y_pred = gnb.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, gnb_y_pred)))\n",
    "print(classification_report(y_test, gnb_y_pred))\n",
    "print(confusion_matrix(y_test, gnb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14.676937818527222 seconds.\n",
      "Accuracty: 0.743272727273\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.95      0.96      0.96       237\n",
      "      coats       0.69      0.70      0.70       270\n",
      "    dresses       0.59      0.73      0.65       240\n",
      "    jackets       0.64      0.62      0.63       250\n",
      "     shirts       0.67      0.60      0.63       227\n",
      "      shoes       0.98      0.97      0.97       253\n",
      "     shorts       0.63      0.81      0.71       251\n",
      "     skirts       0.67      0.59      0.63       266\n",
      "   swimwear       0.81      0.69      0.74       263\n",
      "   trousers       0.85      0.83      0.84       244\n",
      "  underwear       0.78      0.68      0.73       249\n",
      "\n",
      "avg / total       0.75      0.74      0.74      2750\n",
      "\n",
      "[[227   2   1   0   0   1   0   3   0   2   1]\n",
      " [  1 189   7  55   8   0   0   9   0   1   0]\n",
      " [  2   4 176   5   7   1  17  21   5   1   1]\n",
      " [  0  55   6 155  30   0   4   0   0   0   0]\n",
      " [  0  11  20  25 137   0   5   8   2  13   6]\n",
      " [  4   0   1   0   1 246   0   0   0   0   1]\n",
      " [  0   0  11   0   5   0 203  20   3   5   4]\n",
      " [  1   1  46   0   3   0  44 158   3   8   2]\n",
      " [  1   2  11   0   3   2  27   5 181   0  31]\n",
      " [  0   4   9   3   3   0  11  10   1 202   1]\n",
      " [  2   4  12   1   9   2  12   2  29   6 170]]\n"
     ]
    }
   ],
   "source": [
    "# Try KNN\n",
    "tic = time.time()\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_y_pred = knn.predict(X_test)\n",
    "toc = time.time()\n",
    "\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, knn_y_pred)))\n",
    "print(classification_report(y_test, knn_y_pred))\n",
    "print(confusion_matrix(y_test, knn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5.425737142562866 seconds.\n",
      "Accuracty: 0.585090909091\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.89      0.88      0.88       237\n",
      "      coats       0.53      0.54      0.54       270\n",
      "    dresses       0.49      0.47      0.48       240\n",
      "    jackets       0.47      0.46      0.46       250\n",
      "     shirts       0.39      0.41      0.40       227\n",
      "      shoes       0.92      0.92      0.92       253\n",
      "     shorts       0.50      0.57      0.53       251\n",
      "     skirts       0.52      0.50      0.51       266\n",
      "   swimwear       0.56      0.54      0.55       263\n",
      "   trousers       0.70      0.67      0.68       244\n",
      "  underwear       0.50      0.48      0.49       249\n",
      "\n",
      "avg / total       0.59      0.59      0.59      2750\n",
      "\n",
      "[[208   3   0   1   4   4   1   3   4   2   7]\n",
      " [  1 145  13  62  24   2   7   5   0   6   5]\n",
      " [  6  11 114  10  16   2  17  33  13   7  11]\n",
      " [  0  67   6 115  38   0   7   2   0  11   4]\n",
      " [  3  22  16  34  92   3  11  11   8  14  13]\n",
      " [  2   1   2   1   3 233   2   4   0   1   4]\n",
      " [  1   1  14   5  10   0 142  36  20  14   8]\n",
      " [  6   5  35   5  17   2  32 134   9   9  12]\n",
      " [  2   3  12   3   8   1  33   9 143   1  48]\n",
      " [  0   8   5   8  12   3  21   9   8 163   7]\n",
      " [  5   6  18   1  12   4  13  13  52   5 120]]\n"
     ]
    }
   ],
   "source": [
    "# Try decision tree, maybe random forest later\n",
    "tic = time.time()\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_y_pred = dt.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, dt_y_pred)))\n",
    "print(classification_report(y_test, dt_y_pred))\n",
    "print(confusion_matrix(y_test, dt_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8250 samples, validate on 2750 samples\n",
      "Epoch 1/100\n",
      "8250/8250 [==============================] - 1s - loss: 1.8128 - acc: 0.5807 - val_loss: 0.9167 - val_acc: 0.7105\n",
      "Epoch 2/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.6866 - acc: 0.7762 - val_loss: 0.7979 - val_acc: 0.7455\n",
      "Epoch 3/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.5268 - acc: 0.8205 - val_loss: 0.7636 - val_acc: 0.7582\n",
      "Epoch 4/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.4387 - acc: 0.8524 - val_loss: 0.7738 - val_acc: 0.7567\n",
      "Epoch 5/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3716 - acc: 0.8768 - val_loss: 0.7833 - val_acc: 0.7560\n",
      "Epoch 6/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.3205 - acc: 0.8933 - val_loss: 0.8109 - val_acc: 0.7545\n",
      "Epoch 7/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2751 - acc: 0.9122 - val_loss: 0.8137 - val_acc: 0.7651\n",
      "Epoch 8/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2351 - acc: 0.9280 - val_loss: 0.8225 - val_acc: 0.7669\n",
      "Epoch 9/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1921 - acc: 0.9462 - val_loss: 0.8609 - val_acc: 0.7596\n",
      "Epoch 10/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1661 - acc: 0.9559 - val_loss: 0.8905 - val_acc: 0.7633\n",
      "Epoch 11/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1367 - acc: 0.9670 - val_loss: 0.9017 - val_acc: 0.7604\n",
      "Epoch 12/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1125 - acc: 0.9744 - val_loss: 0.9332 - val_acc: 0.7615\n",
      "Epoch 13/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0912 - acc: 0.9823 - val_loss: 0.9764 - val_acc: 0.7575\n",
      "Epoch 14/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0769 - acc: 0.9881 - val_loss: 0.9961 - val_acc: 0.7633\n",
      "Epoch 15/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0617 - acc: 0.9926 - val_loss: 1.0191 - val_acc: 0.7593\n",
      "Epoch 16/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0493 - acc: 0.9961 - val_loss: 1.0522 - val_acc: 0.7640\n",
      "Epoch 17/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0422 - acc: 0.9960 - val_loss: 1.0979 - val_acc: 0.7640\n",
      "Epoch 18/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0341 - acc: 0.9981 - val_loss: 1.1108 - val_acc: 0.7647\n",
      "Epoch 19/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0271 - acc: 0.9985 - val_loss: 1.1228 - val_acc: 0.7662\n",
      "Epoch 20/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0222 - acc: 0.9993 - val_loss: 1.1528 - val_acc: 0.7651\n",
      "Epoch 21/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0193 - acc: 0.9995 - val_loss: 1.1838 - val_acc: 0.7702\n",
      "Epoch 22/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0155 - acc: 0.9999 - val_loss: 1.2032 - val_acc: 0.7705\n",
      "Epoch 23/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0131 - acc: 0.9998 - val_loss: 1.2279 - val_acc: 0.7684\n",
      "Epoch 24/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0225 - acc: 0.9959 - val_loss: 1.2604 - val_acc: 0.7636\n",
      "Epoch 25/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0178 - acc: 0.9976 - val_loss: 1.3017 - val_acc: 0.7589\n",
      "Epoch 26/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0150 - acc: 0.9984 - val_loss: 1.3070 - val_acc: 0.7687\n",
      "Epoch 27/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0284 - acc: 0.9935 - val_loss: 1.3726 - val_acc: 0.7564\n",
      "Epoch 28/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0581 - acc: 0.9863 - val_loss: 1.4491 - val_acc: 0.7473\n",
      "Epoch 29/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0808 - acc: 0.9737 - val_loss: 1.4518 - val_acc: 0.7505\n",
      "Epoch 30/100\n",
      "8250/8250 [==============================] - ETA: 0s - loss: 0.0469 - acc: 0.987 - 0s - loss: 0.0464 - acc: 0.9875 - val_loss: 1.4405 - val_acc: 0.7564\n",
      "Epoch 31/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0239 - acc: 0.9965 - val_loss: 1.4489 - val_acc: 0.7644\n",
      "Epoch 32/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0127 - acc: 0.9983 - val_loss: 1.4545 - val_acc: 0.7684\n",
      "Epoch 33/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0119 - acc: 0.9985 - val_loss: 1.4919 - val_acc: 0.7607\n",
      "Epoch 34/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0043 - acc: 1.0000 - val_loss: 1.4649 - val_acc: 0.7651\n",
      "Epoch 35/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0032 - acc: 1.0000 - val_loss: 1.4755 - val_acc: 0.7644\n",
      "Epoch 36/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0028 - acc: 1.0000 - val_loss: 1.4834 - val_acc: 0.7647\n",
      "Epoch 37/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0025 - acc: 1.0000 - val_loss: 1.4899 - val_acc: 0.7640\n",
      "Epoch 38/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0023 - acc: 1.0000 - val_loss: 1.4973 - val_acc: 0.7636\n",
      "Epoch 39/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0021 - acc: 1.0000 - val_loss: 1.5097 - val_acc: 0.7655\n",
      "Epoch 40/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0019 - acc: 1.0000 - val_loss: 1.5125 - val_acc: 0.7647\n",
      "Epoch 41/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0018 - acc: 1.0000 - val_loss: 1.5192 - val_acc: 0.7622\n",
      "Epoch 42/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0016 - acc: 1.0000 - val_loss: 1.5294 - val_acc: 0.7622\n",
      "Epoch 43/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0015 - acc: 1.0000 - val_loss: 1.5363 - val_acc: 0.7633\n",
      "Epoch 44/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0014 - acc: 1.0000 - val_loss: 1.5420 - val_acc: 0.7629\n",
      "Epoch 45/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0013 - acc: 1.0000 - val_loss: 1.5521 - val_acc: 0.7622\n",
      "Epoch 46/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0012 - acc: 1.0000 - val_loss: 1.5597 - val_acc: 0.7644\n",
      "Epoch 47/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0011 - acc: 1.0000 - val_loss: 1.5730 - val_acc: 0.7629\n",
      "Epoch 48/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0010 - acc: 1.0000 - val_loss: 1.5755 - val_acc: 0.7618\n",
      "Epoch 49/100\n",
      "8250/8250 [==============================] - 0s - loss: 9.4876e-04 - acc: 1.0000 - val_loss: 1.5864 - val_acc: 0.7615\n",
      "Epoch 50/100\n",
      "8250/8250 [==============================] - 0s - loss: 8.7802e-04 - acc: 1.0000 - val_loss: 1.5927 - val_acc: 0.7629\n",
      "Epoch 51/100\n",
      "8250/8250 [==============================] - 0s - loss: 8.1882e-04 - acc: 1.0000 - val_loss: 1.6039 - val_acc: 0.7629\n",
      "Epoch 52/100\n",
      "8250/8250 [==============================] - 0s - loss: 7.5721e-04 - acc: 1.0000 - val_loss: 1.6130 - val_acc: 0.7622\n",
      "Epoch 53/100\n",
      "8250/8250 [==============================] - 0s - loss: 7.0167e-04 - acc: 1.0000 - val_loss: 1.6203 - val_acc: 0.7640\n",
      "Epoch 54/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0015 - acc: 0.9999 - val_loss: 1.7054 - val_acc: 0.7553\n",
      "Epoch 55/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.2314 - acc: 0.9392 - val_loss: 1.8309 - val_acc: 0.7389c\n",
      "Epoch 56/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.1343 - acc: 0.9556 - val_loss: 1.7252 - val_acc: 0.7611\n",
      "Epoch 57/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0260 - acc: 0.9916 - val_loss: 1.6993 - val_acc: 0.7589\n",
      "Epoch 58/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0074 - acc: 0.9989 - val_loss: 1.6828 - val_acc: 0.7644\n",
      "Epoch 59/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0042 - acc: 0.9996 - val_loss: 1.6899 - val_acc: 0.7647\n",
      "Epoch 60/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0029 - acc: 0.9999 - val_loss: 1.6883 - val_acc: 0.7669\n",
      "Epoch 61/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0019 - acc: 1.0000 - val_loss: 1.6953 - val_acc: 0.7658\n",
      "Epoch 62/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0017 - acc: 1.0000 - val_loss: 1.6990 - val_acc: 0.7658\n",
      "Epoch 63/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0015 - acc: 1.0000 - val_loss: 1.7078 - val_acc: 0.7662\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8250/8250 [==============================] - 0s - loss: 0.0013 - acc: 1.0000 - val_loss: 1.7089 - val_acc: 0.7687\n",
      "Epoch 65/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0012 - acc: 1.0000 - val_loss: 1.7139 - val_acc: 0.7665\n",
      "Epoch 66/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0011 - acc: 1.0000 - val_loss: 1.7183 - val_acc: 0.7673\n",
      "Epoch 67/100\n",
      "8250/8250 [==============================] - 1s - loss: 9.9424e-04 - acc: 1.0000 - val_loss: 1.7232 - val_acc: 0.7676\n",
      "Epoch 68/100\n",
      "8250/8250 [==============================] - 0s - loss: 9.1579e-04 - acc: 1.0000 - val_loss: 1.7285 - val_acc: 0.7669\n",
      "Epoch 69/100\n",
      "8250/8250 [==============================] - 0s - loss: 8.4809e-04 - acc: 1.0000 - val_loss: 1.7329 - val_acc: 0.7695\n",
      "Epoch 70/100\n",
      "8250/8250 [==============================] - 0s - loss: 8.1109e-04 - acc: 1.0000 - val_loss: 1.7358 - val_acc: 0.7687\n",
      "Epoch 71/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0011 - acc: 0.9999 - val_loss: 1.7507 - val_acc: 0.7680\n",
      "Epoch 72/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0038 - acc: 0.9993 - val_loss: 1.7820 - val_acc: 0.7665\n",
      "Epoch 73/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0145 - acc: 0.9961 - val_loss: 1.8161 - val_acc: 0.7625\n",
      "Epoch 74/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0453 - acc: 0.9868 - val_loss: 1.9231 - val_acc: 0.7564\n",
      "Epoch 75/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0666 - acc: 0.9790 - val_loss: 1.9485 - val_acc: 0.7476\n",
      "Epoch 76/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0320 - acc: 0.9910 - val_loss: 1.9264 - val_acc: 0.7527\n",
      "Epoch 77/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0143 - acc: 0.9967 - val_loss: 1.9019 - val_acc: 0.7589\n",
      "Epoch 78/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0039 - acc: 0.9992 - val_loss: 1.9048 - val_acc: 0.7571\n",
      "Epoch 79/100\n",
      "8250/8250 [==============================] - 0s - loss: 0.0015 - acc: 0.9999 - val_loss: 1.8917 - val_acc: 0.7582\n",
      "Epoch 80/100\n",
      "8250/8250 [==============================] - 0s - loss: 9.1898e-04 - acc: 1.0000 - val_loss: 1.8936 - val_acc: 0.7578\n",
      "Epoch 81/100\n",
      "8250/8250 [==============================] - 0s - loss: 7.7561e-04 - acc: 1.0000 - val_loss: 1.8939 - val_acc: 0.7585\n",
      "Epoch 82/100\n",
      "8250/8250 [==============================] - 0s - loss: 6.8456e-04 - acc: 1.0000 - val_loss: 1.8951 - val_acc: 0.7596\n",
      "Epoch 83/100\n",
      "8250/8250 [==============================] - 0s - loss: 6.1405e-04 - acc: 1.0000 - val_loss: 1.8961 - val_acc: 0.7593\n",
      "Epoch 84/100\n",
      "8250/8250 [==============================] - 0s - loss: 5.5827e-04 - acc: 1.0000 - val_loss: 1.8984 - val_acc: 0.7604\n",
      "Epoch 85/100\n",
      "8250/8250 [==============================] - 0s - loss: 5.1051e-04 - acc: 1.0000 - val_loss: 1.9006 - val_acc: 0.7585\n",
      "Epoch 86/100\n",
      "8250/8250 [==============================] - 0s - loss: 4.6943e-04 - acc: 1.0000 - val_loss: 1.9015 - val_acc: 0.7600\n",
      "Epoch 87/100\n",
      "8250/8250 [==============================] - 0s - loss: 4.3361e-04 - acc: 1.0000 - val_loss: 1.9036 - val_acc: 0.7596\n",
      "Epoch 88/100\n",
      "8250/8250 [==============================] - 0s - loss: 4.0192e-04 - acc: 1.0000 - val_loss: 1.9058 - val_acc: 0.7589\n",
      "Epoch 89/100\n",
      "8250/8250 [==============================] - 0s - loss: 3.7375e-04 - acc: 1.0000 - val_loss: 1.9085 - val_acc: 0.7600\n",
      "Epoch 90/100\n",
      "8250/8250 [==============================] - 0s - loss: 3.4814e-04 - acc: 1.0000 - val_loss: 1.9100 - val_acc: 0.7604\n",
      "Epoch 91/100\n",
      "8250/8250 [==============================] - 0s - loss: 3.2456e-04 - acc: 1.0000 - val_loss: 1.9126 - val_acc: 0.7593\n",
      "Epoch 92/100\n",
      "8250/8250 [==============================] - 0s - loss: 3.0407e-04 - acc: 1.0000 - val_loss: 1.9145 - val_acc: 0.7593\n",
      "Epoch 93/100\n",
      "8250/8250 [==============================] - 0s - loss: 2.8460e-04 - acc: 1.0000 - val_loss: 1.9173 - val_acc: 0.7600\n",
      "Epoch 94/100\n",
      "8250/8250 [==============================] - 0s - loss: 2.6639e-04 - acc: 1.0000 - val_loss: 1.9184 - val_acc: 0.7607\n",
      "Epoch 95/100\n",
      "8250/8250 [==============================] - 0s - loss: 2.5013e-04 - acc: 1.0000 - val_loss: 1.9207 - val_acc: 0.7611\n",
      "Epoch 96/100\n",
      "8250/8250 [==============================] - 0s - loss: 2.3548e-04 - acc: 1.0000 - val_loss: 1.9239 - val_acc: 0.7604\n",
      "Epoch 97/100\n",
      "8250/8250 [==============================] - 0s - loss: 2.2008e-04 - acc: 1.0000 - val_loss: 1.9247 - val_acc: 0.7604\n",
      "Epoch 98/100\n",
      "8250/8250 [==============================] - 0s - loss: 2.0803e-04 - acc: 1.0000 - val_loss: 1.9291 - val_acc: 0.7618\n",
      "Epoch 99/100\n",
      "8250/8250 [==============================] - 0s - loss: 1.9349e-04 - acc: 1.0000 - val_loss: 1.9323 - val_acc: 0.7611\n",
      "Epoch 100/100\n",
      "8250/8250 [==============================] - 0s - loss: 1.8303e-04 - acc: 1.0000 - val_loss: 1.9341 - val_acc: 0.7611\n",
      "Time: 80.8276789188385 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Try neural network\n",
    "tic = time.time()\n",
    "\n",
    "# Reshape data\n",
    "# Data to numpy arrays\n",
    "npy_X_test = X_test.values\n",
    "npy_X_train = X_train.values\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "onehot_y_train = np_utils.to_categorical(encoded_y_train)\n",
    "onehot_y_test = np_utils.to_categorical(encoded_y_test)\n",
    "\n",
    "# Build model\n",
    "nn = Sequential()\n",
    "nn.add(Dense(100, input_dim=300, activation='relu'))\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train & Test\n",
    "nn.fit(npy_X_train, onehot_y_train,\n",
    "       validation_data=(npy_X_test, onehot_y_test),\n",
    "       epochs=100, batch_size=50)\n",
    "\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From text and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allhand[allhand.columns[4:]]\n",
    "y = allhand_id.hand\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 199.36328291893005 seconds.\n",
      "Accuracty: 0.912363636364\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       1.00      1.00      1.00       237\n",
      "      coats       0.87      0.89      0.88       270\n",
      "    dresses       0.86      0.94      0.90       240\n",
      "    jackets       0.86      0.86      0.86       250\n",
      "     shirts       0.90      0.82      0.86       227\n",
      "      shoes       1.00      1.00      1.00       253\n",
      "     shorts       0.85      0.90      0.87       251\n",
      "     skirts       0.92      0.92      0.92       266\n",
      "   swimwear       0.94      0.90      0.92       263\n",
      "   trousers       0.91      0.96      0.93       244\n",
      "  underwear       0.93      0.84      0.88       249\n",
      "\n",
      "avg / total       0.91      0.91      0.91      2750\n",
      "\n",
      "[[237   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 240   4  22   2   0   0   2   0   0   0]\n",
      " [  0   2 226   0   2   0   8   0   0   1   1]\n",
      " [  0  26   1 215   6   0   1   0   0   1   0]\n",
      " [  0   2   3   9 186   0   5   0   4  13   5]\n",
      " [  0   0   0   0   0 253   0   0   0   0   0]\n",
      " [  0   1   4   2   1   0 225  11   2   3   2]\n",
      " [  0   1   7   1   0   0   7 245   1   3   1]\n",
      " [  0   0   6   0   1   0  10   1 238   0   7]\n",
      " [  0   0   2   1   1   0   1   4   0 235   0]\n",
      " [  0   3   9   1   8   0   7   2   7   3 209]]\n"
     ]
    }
   ],
   "source": [
    "# Try logistic regression\n",
    "tic = time.time()\n",
    "lr = LogisticRegression(random_state=1)\n",
    "lr.fit(X, y)\n",
    "lr_y_pred = lr.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, lr_y_pred)))\n",
    "print(classification_report(y_test, lr_y_pred))\n",
    "print(confusion_matrix(y_test, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 54.0937180519104 seconds.\n",
      "Accuracty: 0.775272727273\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.96      0.94      0.95       237\n",
      "      coats       0.89      0.40      0.56       270\n",
      "    dresses       0.81      0.57      0.67       240\n",
      "    jackets       0.61      0.90      0.72       250\n",
      "     shirts       0.66      0.73      0.69       227\n",
      "      shoes       0.96      0.98      0.97       253\n",
      "     shorts       0.67      0.84      0.75       251\n",
      "     skirts       0.81      0.73      0.77       266\n",
      "   swimwear       0.77      0.83      0.80       263\n",
      "   trousers       0.76      0.91      0.83       244\n",
      "  underwear       0.82      0.72      0.77       249\n",
      "\n",
      "avg / total       0.79      0.78      0.77      2750\n",
      "\n",
      "[[223   0   1   4   2   2   0   1   1   0   3]\n",
      " [  1 109   6 106  29   1   1   9   1   6   1]\n",
      " [  2   2 137   8  19   2  24  12  14  10  10]\n",
      " [  0   7   2 225  10   0   2   1   0   2   1]\n",
      " [  1   2   1  19 165   0   5   1   8  20   5]\n",
      " [  1   1   1   0   0 247   0   0   1   0   2]\n",
      " [  0   0   2   1   2   0 212  16   8   7   3]\n",
      " [  1   0   9   2   5   2  34 193   3  15   2]\n",
      " [  3   0   2   1   2   2  19   1 219   2  12]\n",
      " [  0   0   5   2   4   0   6   3   2 222   0]\n",
      " [  1   1   3   3  12   2  12   1  27   7 180]]\n"
     ]
    }
   ],
   "source": [
    "# Try linear SVM\n",
    "tic = time.time()\n",
    "svc = LinearSVC(random_state=1)\n",
    "svc.fit(X_train, y_train)\n",
    "svc_y_pred = svc.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, svc_y_pred)))\n",
    "print(classification_report(y_test, svc_y_pred))\n",
    "print(confusion_matrix(y_test, svc_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.5294129848480225 seconds.\n",
      "Accuracty: 0.625818181818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.94      0.91      0.93       237\n",
      "      coats       0.73      0.47      0.58       270\n",
      "    dresses       0.60      0.75      0.67       240\n",
      "    jackets       0.52      0.69      0.59       250\n",
      "     shirts       0.72      0.37      0.49       227\n",
      "      shoes       0.90      0.87      0.89       253\n",
      "     shorts       0.60      0.36      0.45       251\n",
      "     skirts       0.85      0.57      0.68       266\n",
      "   swimwear       0.49      0.79      0.60       263\n",
      "   trousers       0.55      0.74      0.63       244\n",
      "  underwear       0.33      0.36      0.34       249\n",
      "\n",
      "avg / total       0.66      0.63      0.62      2750\n",
      "\n",
      "[[216   0   1   3   0   1   0   1   7   0   8]\n",
      " [  1 128   3 102   7   3   2   2   6   1  15]\n",
      " [  1   0 181   3   9   3   5   4   7   8  19]\n",
      " [  2  34   5 173   6   0   2   0   8   6  14]\n",
      " [  0   6  26  35  83   0   4   2  24  28  19]\n",
      " [  0   1   2   0   0 221   0   0  10   1  18]\n",
      " [  0   2  21   4   1   0  90  15  27  69  22]\n",
      " [  1   0  33   4   0   2  19 152  22  15  18]\n",
      " [  2   0   4   2   0   4  12   0 207   6  26]\n",
      " [  3   4   7   5   4   1   7   2   9 181  21]\n",
      " [  3   0  18   3   5  10   9   1  98  13  89]]\n"
     ]
    }
   ],
   "source": [
    "# Try naive bayes\n",
    "tic = time.time()\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_y_pred = gnb.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, gnb_y_pred)))\n",
    "print(classification_report(y_test, gnb_y_pred))\n",
    "print(confusion_matrix(y_test, gnb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26.06452703475952 seconds.\n",
      "Accuracty: 0.743272727273\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.95      0.96      0.96       237\n",
      "      coats       0.69      0.70      0.70       270\n",
      "    dresses       0.59      0.73      0.65       240\n",
      "    jackets       0.64      0.62      0.63       250\n",
      "     shirts       0.67      0.60      0.63       227\n",
      "      shoes       0.98      0.97      0.97       253\n",
      "     shorts       0.63      0.81      0.71       251\n",
      "     skirts       0.67      0.59      0.63       266\n",
      "   swimwear       0.81      0.69      0.74       263\n",
      "   trousers       0.85      0.83      0.84       244\n",
      "  underwear       0.78      0.68      0.73       249\n",
      "\n",
      "avg / total       0.75      0.74      0.74      2750\n",
      "\n",
      "[[227   2   1   0   0   1   0   3   0   2   1]\n",
      " [  1 189   7  55   8   0   0   9   0   1   0]\n",
      " [  2   4 176   5   7   1  17  21   5   1   1]\n",
      " [  0  55   6 155  30   0   4   0   0   0   0]\n",
      " [  0  11  20  25 137   0   5   8   2  13   6]\n",
      " [  4   0   1   0   1 246   0   0   0   0   1]\n",
      " [  0   0  11   0   5   0 203  20   3   5   4]\n",
      " [  1   1  46   0   3   0  44 158   3   8   2]\n",
      " [  1   2  11   0   3   2  27   5 181   0  31]\n",
      " [  0   4   9   3   3   0  11  10   1 202   1]\n",
      " [  2   4  12   1   9   2  12   2  29   6 170]]\n"
     ]
    }
   ],
   "source": [
    "# Try KNN\n",
    "tic = time.time()\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_y_pred = knn.predict(X_test)\n",
    "toc = time.time()\n",
    "\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, knn_y_pred)))\n",
    "print(classification_report(y_test, knn_y_pred))\n",
    "print(confusion_matrix(y_test, knn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 10.122843027114868 seconds.\n",
      "Accuracty: 0.615636363636\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       bags       0.89      0.88      0.89       237\n",
      "      coats       0.53      0.57      0.55       270\n",
      "    dresses       0.53      0.48      0.51       240\n",
      "    jackets       0.49      0.48      0.49       250\n",
      "     shirts       0.44      0.48      0.46       227\n",
      "      shoes       0.90      0.92      0.91       253\n",
      "     shorts       0.54      0.58      0.56       251\n",
      "     skirts       0.59      0.57      0.58       266\n",
      "   swimwear       0.67      0.62      0.64       263\n",
      "   trousers       0.67      0.66      0.66       244\n",
      "  underwear       0.54      0.53      0.54       249\n",
      "\n",
      "avg / total       0.62      0.62      0.62      2750\n",
      "\n",
      "[[208   2   1   1   5   4   2   2   3   2   7]\n",
      " [  0 154   8  62  24   1   7   2   2   6   4]\n",
      " [  3  13 116  11  19   4  16  32   7   6  13]\n",
      " [  1  64   9 120  30   0   8   1   1  10   6]\n",
      " [  2  22  18  32 110   3   5   7   5  16   7]\n",
      " [  7   1   3   0   2 232   3   1   0   2   2]\n",
      " [  1   5  12   4  19   0 146  24  14  13  13]\n",
      " [  2   7  26   0  11   4  30 152   6  13  15]\n",
      " [  4   2   6   3   8   1  21  11 163   5  39]\n",
      " [  0  14   9   7  13   2  23  10   1 160   5]\n",
      " [  5   5  11   4  11   7  11  16  42   5 132]]\n"
     ]
    }
   ],
   "source": [
    "# Try decision tree, maybe random forest later\n",
    "tic = time.time()\n",
    "dt = DecisionTreeClassifier(random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_y_pred = dt.predict(X_test)\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n",
    "print('Accuracty: ' + str(accuracy_score(y_test, dt_y_pred)))\n",
    "print(classification_report(y_test, dt_y_pred))\n",
    "print(confusion_matrix(y_test, dt_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8250 samples, validate on 2750 samples\n",
      "Epoch 1/100\n",
      "8250/8250 [==============================] - 2s - loss: 1.0649 - acc: 0.6895 - val_loss: 0.7552 - val_acc: 0.7604\n",
      "Epoch 2/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.5092 - acc: 0.8299 - val_loss: 0.6907 - val_acc: 0.7825\n",
      "Epoch 3/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.3588 - acc: 0.8827 - val_loss: 0.6679 - val_acc: 0.7884\n",
      "Epoch 4/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.2595 - acc: 0.9158 - val_loss: 0.6723 - val_acc: 0.7942\n",
      "Epoch 5/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.1870 - acc: 0.9433 - val_loss: 0.6856 - val_acc: 0.7891\n",
      "Epoch 6/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.1335 - acc: 0.9634 - val_loss: 0.6796 - val_acc: 0.8033\n",
      "Epoch 7/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0913 - acc: 0.9812 - val_loss: 0.6836 - val_acc: 0.8080\n",
      "Epoch 8/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0611 - acc: 0.9909 - val_loss: 0.7343 - val_acc: 0.8022\n",
      "Epoch 9/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0418 - acc: 0.9962 - val_loss: 0.7153 - val_acc: 0.8105\n",
      "Epoch 10/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0292 - acc: 0.9981 - val_loss: 0.7559 - val_acc: 0.8065\n",
      "Epoch 11/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0208 - acc: 0.9990 - val_loss: 0.7536 - val_acc: 0.8091\n",
      "Epoch 12/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0217 - acc: 0.9977 - val_loss: 0.7783 - val_acc: 0.8065\n",
      "Epoch 13/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0282 - acc: 0.9955 - val_loss: 0.8142 - val_acc: 0.8029\n",
      "Epoch 14/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0385 - acc: 0.9915 - val_loss: 0.8873 - val_acc: 0.8011\n",
      "Epoch 15/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0633 - acc: 0.9858 - val_loss: 0.9545 - val_acc: 0.8000\n",
      "Epoch 16/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0570 - acc: 0.9862 - val_loss: 0.9508 - val_acc: 0.7942\n",
      "Epoch 17/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0615 - acc: 0.9825 - val_loss: 0.9728 - val_acc: 0.7989\n",
      "Epoch 18/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0455 - acc: 0.9879 - val_loss: 1.0188 - val_acc: 0.7931\n",
      "Epoch 19/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0293 - acc: 0.9933 - val_loss: 0.9763 - val_acc: 0.8004\n",
      "Epoch 20/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0127 - acc: 0.9987 - val_loss: 0.9740 - val_acc: 0.8033\n",
      "Epoch 21/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0069 - acc: 0.9998 - val_loss: 0.9750 - val_acc: 0.8062\n",
      "Epoch 22/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0059 - acc: 0.9998 - val_loss: 0.9818 - val_acc: 0.8065\n",
      "Epoch 23/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0061 - acc: 0.9995 - val_loss: 0.9973 - val_acc: 0.8062\n",
      "Epoch 24/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0155 - acc: 0.9975 - val_loss: 1.0001 - val_acc: 0.8051\n",
      "Epoch 25/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0095 - acc: 0.9989 - val_loss: 1.0320 - val_acc: 0.7993\n",
      "Epoch 26/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0194 - acc: 0.9960 - val_loss: 1.0512 - val_acc: 0.8015\n",
      "Epoch 27/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0208 - acc: 0.9949 - val_loss: 1.0561 - val_acc: 0.8036\n",
      "Epoch 28/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0511 - acc: 0.9870 - val_loss: 1.1828 - val_acc: 0.7887\n",
      "Epoch 29/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0917 - acc: 0.9716 - val_loss: 1.3596 - val_acc: 0.7844\n",
      "Epoch 30/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0905 - acc: 0.9727 - val_loss: 1.2111 - val_acc: 0.7978\n",
      "Epoch 31/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0356 - acc: 0.9905 - val_loss: 1.2097 - val_acc: 0.7978\n",
      "Epoch 32/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0125 - acc: 0.9981 - val_loss: 1.1681 - val_acc: 0.8087\n",
      "Epoch 33/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0077 - acc: 0.9990 - val_loss: 1.1889 - val_acc: 0.8102\n",
      "Epoch 34/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0082 - acc: 0.9992 - val_loss: 1.1758 - val_acc: 0.8098\n",
      "Epoch 35/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0063 - acc: 0.9995 - val_loss: 1.1965 - val_acc: 0.8073\n",
      "Epoch 36/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0070 - acc: 0.9992 - val_loss: 1.2248 - val_acc: 0.8069\n",
      "Epoch 37/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0075 - acc: 0.9994 - val_loss: 1.1965 - val_acc: 0.8113\n",
      "Epoch 38/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0045 - acc: 0.9998 - val_loss: 1.1974 - val_acc: 0.8120\n",
      "Epoch 39/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0044 - acc: 0.9998 - val_loss: 1.2006 - val_acc: 0.8124\n",
      "Epoch 40/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0044 - acc: 0.9998 - val_loss: 1.2037 - val_acc: 0.8120\n",
      "Epoch 41/100\n",
      "8250/8250 [==============================] - 2s - loss: 0.0043 - acc: 0.9998 - val_loss: 1.2068 - val_acc: 0.8131\n",
      "Epoch 42/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0043 - acc: 0.9998 - val_loss: 1.2097 - val_acc: 0.8131\n",
      "Epoch 43/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0043 - acc: 0.9998 - val_loss: 1.2122 - val_acc: 0.8124\n",
      "Epoch 44/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0042 - acc: 0.9998 - val_loss: 1.2170 - val_acc: 0.8135\n",
      "Epoch 45/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0042 - acc: 0.9998 - val_loss: 1.2190 - val_acc: 0.8135\n",
      "Epoch 46/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0042 - acc: 0.9998 - val_loss: 1.2233 - val_acc: 0.8142\n",
      "Epoch 47/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0042 - acc: 0.9998 - val_loss: 1.2262 - val_acc: 0.8138\n",
      "Epoch 48/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.2289 - val_acc: 0.8138\n",
      "Epoch 49/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.2318 - val_acc: 0.8145\n",
      "Epoch 50/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.2343 - val_acc: 0.8135\n",
      "Epoch 51/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.2388 - val_acc: 0.8138\n",
      "Epoch 52/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.2411 - val_acc: 0.8145\n",
      "Epoch 53/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.2446 - val_acc: 0.8145\n",
      "Epoch 54/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.2474 - val_acc: 0.8149\n",
      "Epoch 55/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.2506 - val_acc: 0.8153\n",
      "Epoch 56/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.2544 - val_acc: 0.8145\n",
      "Epoch 57/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.2563 - val_acc: 0.8160\n",
      "Epoch 58/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.2602 - val_acc: 0.8153\n",
      "Epoch 59/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.2637 - val_acc: 0.8156\n",
      "Epoch 60/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.2675 - val_acc: 0.8149\n",
      "Epoch 61/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.2700 - val_acc: 0.8142\n",
      "Epoch 62/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.2743 - val_acc: 0.8135\n",
      "Epoch 63/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0739 - acc: 0.9910 - val_loss: 1.8602 - val_acc: 0.7538\n",
      "Epoch 64/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.3344 - acc: 0.9181 - val_loss: 1.4119 - val_acc: 0.7702\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8250/8250 [==============================] - 1s - loss: 0.0619 - acc: 0.9805 - val_loss: 1.4296 - val_acc: 0.7931\n",
      "Epoch 66/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0155 - acc: 0.9967 - val_loss: 1.3659 - val_acc: 0.7978\n",
      "Epoch 67/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0066 - acc: 0.9995 - val_loss: 1.3635 - val_acc: 0.8000\n",
      "Epoch 68/100\n",
      "8250/8250 [==============================] - 2s - loss: 0.0056 - acc: 0.9996 - val_loss: 1.3753 - val_acc: 0.7975\n",
      "Epoch 69/100\n",
      "8250/8250 [==============================] - 2s - loss: 0.0060 - acc: 0.9994 - val_loss: 1.3693 - val_acc: 0.8004\n",
      "Epoch 70/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0047 - acc: 0.9998 - val_loss: 1.3737 - val_acc: 0.7996\n",
      "Epoch 71/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0046 - acc: 0.9998 - val_loss: 1.3772 - val_acc: 0.7996\n",
      "Epoch 72/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0045 - acc: 0.9998 - val_loss: 1.3810 - val_acc: 0.8000\n",
      "Epoch 73/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0044 - acc: 0.9998 - val_loss: 1.3857 - val_acc: 0.7993\n",
      "Epoch 74/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0044 - acc: 0.9998 - val_loss: 1.3885 - val_acc: 0.7996\n",
      "Epoch 75/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0043 - acc: 0.9998 - val_loss: 1.3919 - val_acc: 0.8004\n",
      "Epoch 76/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0043 - acc: 0.9998 - val_loss: 1.3958 - val_acc: 0.8004\n",
      "Epoch 77/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0042 - acc: 0.9998 - val_loss: 1.3991 - val_acc: 0.8004\n",
      "Epoch 78/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0042 - acc: 0.9998 - val_loss: 1.4029 - val_acc: 0.8015\n",
      "Epoch 79/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0042 - acc: 0.9998 - val_loss: 1.4059 - val_acc: 0.8022\n",
      "Epoch 80/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0042 - acc: 0.9998 - val_loss: 1.4101 - val_acc: 0.8022\n",
      "Epoch 81/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.4130 - val_acc: 0.8029\n",
      "Epoch 82/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.4167 - val_acc: 0.8029\n",
      "Epoch 83/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.4199 - val_acc: 0.8033\n",
      "Epoch 84/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.4234 - val_acc: 0.8036\n",
      "Epoch 85/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.4277 - val_acc: 0.8040\n",
      "Epoch 86/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.4302 - val_acc: 0.8033\n",
      "Epoch 87/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0041 - acc: 0.9998 - val_loss: 1.4336 - val_acc: 0.8044\n",
      "Epoch 88/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4370 - val_acc: 0.8044\n",
      "Epoch 89/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4399 - val_acc: 0.8040\n",
      "Epoch 90/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4434 - val_acc: 0.8055\n",
      "Epoch 91/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4469 - val_acc: 0.8047\n",
      "Epoch 92/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4500 - val_acc: 0.8058\n",
      "Epoch 93/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4530 - val_acc: 0.8055\n",
      "Epoch 94/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4564 - val_acc: 0.8058\n",
      "Epoch 95/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4597 - val_acc: 0.8062\n",
      "Epoch 96/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4632 - val_acc: 0.8069\n",
      "Epoch 97/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4664 - val_acc: 0.8080\n",
      "Epoch 98/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4703 - val_acc: 0.8076\n",
      "Epoch 99/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4729 - val_acc: 0.8084\n",
      "Epoch 100/100\n",
      "8250/8250 [==============================] - 1s - loss: 0.0040 - acc: 0.9998 - val_loss: 1.4760 - val_acc: 0.8076\n",
      "Time: 159.06521105766296 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Try neural network\n",
    "tic = time.time()\n",
    "\n",
    "# Reshape data\n",
    "# Data to numpy arrays\n",
    "npy_X_test = X_test.values\n",
    "npy_X_train = X_train.values\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_test = encoder.transform(y_test)\n",
    "onehot_y_train = np_utils.to_categorical(encoded_y_train)\n",
    "onehot_y_test = np_utils.to_categorical(encoded_y_test)\n",
    "\n",
    "# Build model\n",
    "nn = Sequential()\n",
    "nn.add(Dense(300, input_dim=600, activation='relu'))\n",
    "nn.add(Dense(11, activation='softmax'))\n",
    "nn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train & Test\n",
    "nn.fit(npy_X_train, onehot_y_train,\n",
    "       validation_data=(npy_X_test, onehot_y_test),\n",
    "       epochs=100, batch_size=50)\n",
    "\n",
    "toc = time.time()\n",
    "print('Time: ' + str(toc - tic) + ' seconds.' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
